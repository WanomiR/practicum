{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68274583-55ca-4f38-90a2-2523cbce2d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Movie reviews prediction\n",
    "NLP | Binary classification | NLTK • spaCy • BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd5295-4860-4036-8f30-82a7018e2d49",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fc6ed-774d-44e7-a2f7-c2ee6ff2272e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b170f3-ed3a-45aa-a947-861d25980042",
   "metadata": {},
   "source": [
    "### The objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdff6f-5e6b-4d91-9882-8d1f50cb032b",
   "metadata": {},
   "source": [
    "The Film Junky Union, a new edgy community for classic movie enthusiasts, is developing a system for filtering and categorizing movie reviews. The goal is to train a model to automatically detect negative reviews. For this task will be used a dataset of IMBD movie reviews with polarity labeling to build a model for classifying positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e642d-12fe-4e92-a087-0549cf9ed643",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dd95b-3fb1-45b8-8f6e-383b308f334e",
   "metadata": {},
   "source": [
    "Here's the description of the fields selected for this task:\n",
    "- `review`: the review text\n",
    "- `pos`: the target, `0` for negative and `1` for positive\n",
    "- `ds_part`: `train`/`test` for the train/test part of dataset, correspondingly  \n",
    "\n",
    "*The data was provided by Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55519fa9-3b70-4eed-9749-e03ce47edc7f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bc0b1-c5f5-45a4-a38f-b17bdd5db8ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fa0803-4533-4fcd-8201-6a1a0abeb291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# import math\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d818accf-2db9-4644-a605-200eca995fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n",
    "sns.set_palette(\"mako\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# this one is to use progress_apply\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5c008-2c0f-418f-9555-a0d498c9176a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b57b7a-b793-4ac6-8bb2-596edae620a1",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd818e6e-6928-480f-add7-9aed48dee71e",
   "metadata": {},
   "source": [
    "### Read and look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dd4256-a0ad-4ff1-a823-2fbd017d65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/imdb_reviews.tsv\", sep=\"\\t\",\n",
    "                   usecols=[\"review\",\"pos\",\"ds_part\",\"start_year\",\"end_year\",\"tconst\", \"is_adult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc795dd-bf44-45d5-813a-aa28533ac1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47331 entries, 0 to 47330\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tconst      47331 non-null  object\n",
      " 1   start_year  47331 non-null  int64 \n",
      " 2   end_year    47331 non-null  object\n",
      " 3   is_adult    47331 non-null  int64 \n",
      " 4   review      47331 non-null  object\n",
      " 5   pos         47331 non-null  int64 \n",
      " 6   ds_part     47331 non-null  object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6f9c619-5870-4f9e-a1c7-bf724aa6d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>is_adult</th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>ds_part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34986</th>\n",
       "      <td>tt0440963</td>\n",
       "      <td>2007</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't understand people. Why is it that this...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36916</th>\n",
       "      <td>tt0080731</td>\n",
       "      <td>1980</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>Even if one didn't realize that Sellers was in...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>tt0450951</td>\n",
       "      <td>2005</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "      <td>I had heard interesting critics on this movie....</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tconst  start_year end_year  is_adult  \\\n",
       "34986  tt0440963        2007       \\N         0   \n",
       "36916  tt0080731        1980       \\N         0   \n",
       "169    tt0450951        2005       \\N         0   \n",
       "\n",
       "                                                  review  pos ds_part  \n",
       "34986  I don't understand people. Why is it that this...    0   train  \n",
       "36916  Even if one didn't realize that Sellers was in...    0   train  \n",
       "169    I had heard interesting critics on this movie....    1    test  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f490649-aa22-4818-9d18-6960fe799d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>30%</th>\n",
       "      <th>40%</th>\n",
       "      <th>50%</th>\n",
       "      <th>60%</th>\n",
       "      <th>70%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start_year</th>\n",
       "      <td>47331.0</td>\n",
       "      <td>1989.631235</td>\n",
       "      <td>19.600364</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_adult</th>\n",
       "      <td>47331.0</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>47331.0</td>\n",
       "      <td>0.498954</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean        std     min     10%     20%     30%  \\\n",
       "start_year  47331.0  1989.631235  19.600364  1894.0  1957.0  1978.0  1986.0   \n",
       "is_adult    47331.0     0.001732   0.041587     0.0     0.0     0.0     0.0   \n",
       "pos         47331.0     0.498954   0.500004     0.0     0.0     0.0     0.0   \n",
       "\n",
       "               40%     50%     60%     70%     80%     90%     max  \n",
       "start_year  1993.0  1998.0  2000.0  2003.0  2005.0  2006.0  2010.0  \n",
       "is_adult       0.0     0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "pos            0.0     0.0     1.0     1.0     1.0     1.0     1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(percentiles=np.arange(0.1, 1, 0.1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae62ae0e-85af-4f9d-ac7d-6697f05e3ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tconst</th>\n",
       "      <td>47331</td>\n",
       "      <td>6648</td>\n",
       "      <td>tt0067445</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_year</th>\n",
       "      <td>47331</td>\n",
       "      <td>60</td>\n",
       "      <td>\\N</td>\n",
       "      <td>45052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>47331</td>\n",
       "      <td>47240</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ds_part</th>\n",
       "      <td>47331</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>23796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count unique                                                top  \\\n",
       "tconst    47331   6648                                          tt0067445   \n",
       "end_year  47331     60                                                 \\N   \n",
       "review    47331  47240  Loved today's show!!! It was a variety and not...   \n",
       "ds_part   47331      2                                              train   \n",
       "\n",
       "           freq  \n",
       "tconst       30  \n",
       "end_year  45052  \n",
       "review        5  \n",
       "ds_part   23796  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include=\"O\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966ba52-243e-4242-b04d-37858527cb41",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- The `end_year` column has an object dtype due to having \"\\N\" string;\n",
    "- The are only two rows where values are explicitly missing;\n",
    "- The `rating` values distribution seem to be right-skewed;\n",
    "- The target variable `pos` is fairly class-balanced;\n",
    "- According to [IMDb site](https://www.imdb.com/interfaces/#:~:text=tconst%20(string)%20%2D-,alphanumeric%20unique%20identifier%20of%20the%20title,-directors%20(array%20of), `tsconst` is a unique identifier of the movie title;\n",
    "- Several review texts are duplicated;\n",
    "\n",
    "*Fun notice:*\n",
    "- More than 90% of the reviewers are not adults :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a33af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The data was loaded and inspected\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd595d59-2bda-4b71-b802-a48f8d4dcf53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507bcaa-5915-414b-a436-d39fc5ea4ab9",
   "metadata": {},
   "source": [
    "Duplicate review texts won't bring new information to a model. Check if there are any duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43dd8c-1f78-48a3-b41b-90999c57d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many duplicated reviews there are\n",
    "data.duplicated(subset=[\"review\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852ac0b-d14a-49b1-8df9-15a0eac3b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these reviews were evaluatate similarly\n",
    "data.duplicated(subset=[\"review\",\"pos\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa97ac-873b-40de-bca7-a470dab4a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with duplicated reviews\n",
    "data.drop_duplicates(subset=[\"review\"], inplace=True)\n",
    "\n",
    "# reset the dataframe index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "# check the number of rows left\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35618ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Great, duplicate entries were removed\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae2f83-6259-41c7-9620-ddb21eae482f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881a1aa-89b4-48ca-95c8-014a987fbe2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d051b-ecd2-4f03-8a2a-5666f2dd4a63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of movies/reviews over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4395f8d-a145-4729-abb8-c7afa15c2b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the number of movies for every year\n",
    "n_movies = (\n",
    "    data[[\"tconst\",\"start_year\"]] # select only the unique title identifier and start year\n",
    "    .drop_duplicates()[\"start_year\"] # drop duplicated rows and select only the column with years\n",
    "    .value_counts() # count the number of movies realeased each year\n",
    "    .sort_index() # sort the series in ascending order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81728e0-b299-4ba2-a152-3a6c8a6366a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the nuber of reviews made every year\n",
    "n_reviews = (\n",
    "    data.groupby([\"start_year\",\"pos\"])[\"pos\"]\n",
    "    .count()\n",
    "    .unstack() # make positive and negative reviews to be separate columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de787f76-7a59-4c28-a4a0-39f4b9773795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calulate the moving average for reviews per movie ratio\n",
    "ratio = (\n",
    "    (data[\"start_year\"].value_counts().sort_index() / n_movies)\n",
    "    .reset_index(drop=True)\n",
    "    .rolling(5)\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380cf5c-de10-4da0-9f61-ccf7407b8151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# plot the number of movies realeased every year\n",
    "ax = axs[0] \n",
    "n_movies.plot(kind=\"bar\", ax=ax, color=\"MediumPurple\")\n",
    "ax.set_title(\"Number of Movies Over Years\")\n",
    "ax.set_ylabel(\"Number of Movies\")\n",
    "ax.set_xticklabels(n_movies.index,fontsize=8)\n",
    "\n",
    "# plot the number of reviews\n",
    "ax = axs[1]\n",
    "n_reviews.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "ax.set_title(\"Number of Reviews Over Years\")\n",
    "ax.set_ylabel(\"Number of Reviews\")\n",
    "ax.set_xlabel(\"Start Year\")\n",
    "ax.set_xticklabels(n_movies.index,fontsize=8)\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, [\"negative\", \"positive\"], loc=\"best\")\n",
    "\n",
    "# plot the rolling reviews per movie ratio\n",
    "ax_t = ax.twinx()\n",
    "ratio.plot(color=\"MediumPurple\", label=\"Revies per movie (5 years avg)\", grid=False)\n",
    "ax_t.set_ylabel(\"Reviews per Movie Ratio\")\n",
    "lines, labels = ax_t.get_legend_handles_labels()\n",
    "ax_t.legend(lines, labels, loc=\"center left\")\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9942e1-b067-4e7b-8e06-2fd0132fb9b4",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "The average number of reviews per movie increases over years, although not as drastically as the total number of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e202a-6434-4049-bcac-2f8a8cb78279",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reviews number distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95216e9-98fa-499d-b14b-3e7426721aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(16, 4))\n",
    "fig.suptitle(\"Reviews Per Movie\", fontsize=14)\n",
    "\n",
    "ax = axs[0]\n",
    "(data\n",
    " .groupby(\"tconst\")[\"review\"] # group the data by unique movie identifiers\n",
    " .count() # count the number of reviews per each movie\n",
    " .value_counts() # count the repeating number of reviews\n",
    " .sort_index() # sort values befor plotting\n",
    " .plot(kind=\"bar\", ax=axs[0])\n",
    ")\n",
    "ax.set_title(\"Bar Plot\")\n",
    "ax.set_xlabel(\"Number of Reviews\")\n",
    "ax.set_ylabel(\"Quantity\")\n",
    "\n",
    "ax = axs[1]\n",
    "(data\n",
    " .groupby(\"tconst\")[\"review\"]\n",
    " .count()\n",
    " .plot(kind=\"kde\", ax=axs[1])\n",
    ")\n",
    "ax.set_xlim([-5, 35])\n",
    "ax.set_xlabel(\"Number of Reviews\")\n",
    "ax.set_title(\"KDE Plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45085f9-8452-4032-af2d-dc09a637c63c",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "We see that mostly movies have few reviews, though we have a spike in number of movies with 30 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93314f11-8253-468b-bc41-f56be2bb1d87",
   "metadata": {},
   "source": [
    "### Classes balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e73e2e-058e-487b-8fdf-3a685580d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the quantity of each target class in the train data\n",
    "data[data[\"ds_part\"] == \"train\"][\"pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2a24d-c332-480a-a9eb-fea5cfac9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"ds_part\"] == \"test\"][\"pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96db4b-deee-4fed-834e-391b7caf7f35",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "The target classes are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830647aa-08a2-40f2-8038-f9631f71138a",
   "metadata": {},
   "source": [
    "### Ratings distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581c26b-8595-4a65-831e-a287845d3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16,4))\n",
    "plt.suptitle(\"Ratings Distribution\", fontsize=14)\n",
    "\n",
    "for axis, ds_part in ((0, \"train\"), (1, \"test\")):\n",
    "    \n",
    "    ax = axs[axis]\n",
    "    \n",
    "    ratings = (data.query(\"ds_part == @ds_part\")[\"rating\"]\n",
    "               .value_counts().sort_index())\n",
    "    \n",
    "    ratings = (ratings\n",
    "               .reindex(index=np.arange(min(ratings.index.min(), 1), max(ratings.index.max(), 11)))\n",
    "               .fillna(0))\n",
    "       \n",
    "    ratings.plot.bar(ax=ax)\n",
    "    ax.set_ylabel(\"Quantity\")\n",
    "    ax.set_xlabel(\"Rating value\")\n",
    "    ax.set_ylim([0, 5000])\n",
    "    ax.set_title(\"{} data\".format(ds_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9a6ce-6128-4c6c-8a22-8ef0f4d919e3",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "We have an even distribution of ratings between the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27acbfb-a14e-4e8b-849e-9055052a4cef",
   "metadata": {},
   "source": [
    "### Neg vs Pos reviews distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23362ba8-9a01-42ea-8180-f3a5af4e8a7a",
   "metadata": {},
   "source": [
    "Distribution of negative and positive reviews over the years for two parts of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c6951-d8c8-4d70-83ac-1bc82797140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw={\"width_ratios\":(3,2), \"height_ratios\":(1,1)})\n",
    "\n",
    "for axis, ds_part in ((0, \"train\"), (1, \"test\")):\n",
    "    \n",
    "    ax = axs[axis][0]\n",
    "    reviews = (data\n",
    "              .query(\"ds_part == @ds_part\")\n",
    "              .groupby([\"start_year\", \"pos\"])[\"pos\"]\n",
    "              .count()\n",
    "              .unstack()\n",
    "             )\n",
    "    reviews.index = reviews.index.astype(\"int\")\n",
    "    reviews = reviews.reindex(\n",
    "        index=np.arange(reviews.index.min(), max(reviews.index.max(), 2015))\n",
    "    ).fillna(0)\n",
    "    \n",
    "    reviews.plot(kind=\"bar\", stacked=True, ax=ax, grid=False)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    ax.set_title(\"The {} set: distribution of different polarities per movie.\".format(ds_part))\n",
    "    ax.set_xlabel(\"Timeline\")\n",
    "    ax.set_ylabel(\"Number of reviews\")\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, [\"negative\", \"positive\"], loc=\"best\")\n",
    "    \n",
    "    ax = axs[axis][1]\n",
    "    reviews = (data\n",
    "               .query(\"ds_part == @ds_part\")\n",
    "               .groupby([\"tconst\",\"pos\"])[\"pos\"]\n",
    "               .count()\n",
    "               .unstack()\n",
    "              )\n",
    "    \n",
    "    sns.kdeplot(reviews[0], color=\"blue\", label=\"negative\", ax=ax)\n",
    "    sns.kdeplot(reviews[1], color=\"green\", label=\"positive\", ax=ax)\n",
    "    ax.set_title(\"The {} set: distribution of different polarities per movie\".format(ds_part))\n",
    "    ax.set_xlabel(\"Number of reviews\")\n",
    "    ax.legend()\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245784a-e0dd-4e6c-bd87-2e69b2b98c31",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "As we can see from the graphs, the test and training sets have fairly even representation for all features and each target class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669adaaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Very well, you explored the data and made some interesting observations\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25eb23-aecc-48f4-a312-2a45b67b86b3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee5bca-c62a-4b50-be09-e9edf9d17c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1e035-f822-41b4-82e7-41be74035328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534fcdc-be3f-4c53-bf65-d4d01dd8d336",
   "metadata": {
    "tags": []
   },
   "source": [
    "We assume all models in this project accept texts in lowercase and without any digits, punctuation marks, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c818664-8dd0-4da9-b99f-c80f5e24e8b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Procedure\n",
    "<a id=\"evaluation-procedure\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d25c3-447d-487f-93b0-056584479368",
   "metadata": {},
   "source": [
    "Compose an evaluation routine which will be used for all models in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4be4b4-e61d-4bc1-abd1-2b87d3138ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \"\"\"\n",
    "    Takes in a model, features and ...\n",
    "    \"\"\"\n",
    "    eval_stats = {}\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    \n",
    "    for ds_part, features, target in ((\"train\", train_features, train_target), (\"test\", test_features, test_target)):\n",
    "        \n",
    "        eval_stats[ds_part] = {}\n",
    "        color = \"gold\" if ds_part == \"train\" else \"steelblue\"\n",
    "        \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:,1]\n",
    "            \n",
    "        # F1 Score\n",
    "        ax = axs[0]\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        \n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        accuracies = [metrics.accuracy_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        max_accuracy_idx = np.argmax(accuracies)\n",
    "        eval_stats[ds_part]['F1'] = f1_scores[max_f1_score_idx]\n",
    "        eval_stats[ds_part]['Accuracy'] = accuracies[max_accuracy_idx]\n",
    "        \n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, \n",
    "                label=f'{ds_part}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # setting the point for the best thershold\n",
    "        ax.plot(f1_thresholds[max_f1_score_idx], f1_scores[max_f1_score_idx], color=\"green\", marker='o', markersize=7)\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in np.arange(0.1, 1.01, 0.1):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=5)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'F1 Score')\n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)\n",
    "        eval_stats[ds_part][\"ROC AUC\"] = roc_auc\n",
    "        \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{ds_part}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in np.arange(0.1, 1.01, 0.1):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=5)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'ROC Curve')\n",
    "        \n",
    "        # Precision Recall Curve\n",
    "        ax = axs[2]\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        ap_score = metrics.average_precision_score(target, pred_proba) \n",
    "        eval_stats[ds_part][\"APS\"] = ap_score\n",
    "        \n",
    "        ax.plot(recall, precision, color=color, label=f'{ds_part}, AP={ap_score:.2f}') \n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in np.arange(0.1, 1.01, 0.1):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=5)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([0.48, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title('Precision Recall Curve')        \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    return df_eval_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b6d3f-7716-4a29-b7fd-18b26ed1c5e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14c4bb-bb1d-4cd5-8cc3-c7fe05570319",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3743138-7d51-4c62-ab1f-a686b5673400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new colum with a normalized review text\n",
    "data[\"review_norm\"] = [\n",
    "    \" \".join( # split and join to remove extra spaces made after substitution\n",
    "        re.sub(r\"[^a-z']\",\" \", text.lower()).split() # substitute all characters \n",
    "                                                     # that don't fit the pattern\n",
    "    ) \n",
    "    for text \n",
    "    in data[\"review\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d4185-def4-4cad-8cb7-04e1c6d3da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how normalization has worked\n",
    "for index in data.sample(5).index:\n",
    "    print(\" RAW: \", data.loc[index, \"review\"][:100])\n",
    "    print(\"NORM: \", data.loc[index, \"review_norm\"][:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c2549",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Normalization was done successfully\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230822d0-bf15-4df6-bbf0-35119a294c68",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lemmatize with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af17559-35ec-4e0e-a006-d2c3688b4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize texts with NLTK package\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    text_lemmatized = \" \".join( # join lemmatized words back into text\n",
    "        [lemmatizer.lemmatize(token) for token in word_tokenize(text)] # get lemmas for each word in the text\n",
    "    )\n",
    "    return text_lemmatized\n",
    "\n",
    "data[\"lemmatized_nltk\"] = data[\"review_norm\"].apply(lemmatize_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a68a-4f22-49a9-b595-7a1ef6984ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how lemmatization has worked\n",
    "for index in data.sample(5).index:\n",
    "    print(\" NORM: \", data.loc[index, \"review_norm\"][:100])\n",
    "    print(\"LEMMA: \", data.loc[index, \"lemmatized_nltk\"][:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ec9f0-2945-4126-84f6-7399a7c6d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create masks for selecting the train and the test set\n",
    "mask_train = data[\"ds_part\"] == \"train\"\n",
    "mask_test = data[\"ds_part\"] == \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf04f5-3e51-4185-9b2c-90aff0d1e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize texts\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "vectorizer_1 = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "train_features_1 = vectorizer_1.fit_transform(data[mask_train][\"lemmatized_nltk\"])\n",
    "test_features_1 = vectorizer_1.transform(data[mask_test][\"lemmatized_nltk\"])\n",
    "\n",
    "words_1 = vectorizer_1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc671bb-2250-482b-8724-8d2a226a9a0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lemmatize with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f85520-929e-45a1-b3cf-148bd5caf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "\n",
    "data[\"lemmatized_spacy\"] = [\n",
    "    \" \".join(\n",
    "        [token.lemma_ for token in nlp(text) if not token.is_stop]\n",
    "    )\n",
    "    for text\n",
    "    in data[\"review_norm\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657e7b6-4095-48a2-9693-74d7219b9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how lemmatization has worked\n",
    "for index in data.sample(5).index:\n",
    "    print(\" NORM: \", data.loc[index, \"review_norm\"][:100])\n",
    "    print(\"LEMMA: \", data.loc[index, \"lemmatized_spacy\"][:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059df94-ebcd-4f9a-9c1a-40fbd4746de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize texts\n",
    "\n",
    "vectorizer_2 = TfidfVectorizer() # we already excluded stop words\n",
    "\n",
    "train_features_2 = vectorizer_2.fit_transform(data[mask_train][\"lemmatized_spacy\"])\n",
    "test_features_2 = vectorizer_2.transform(data[mask_test][\"lemmatized_spacy\"])\n",
    "\n",
    "words_2 = vectorizer_2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c598b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "It's nice that you tried lemmatization! TF-IDF vectorizer was applied correctly\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147f846-47aa-4382-9a82-0d4d611ee98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### N-grams vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12938996-eaac-4ab0-a9ab-f207469c30fc",
   "metadata": {},
   "source": [
    "Try to train the model with n_grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee6cde-831a-4714-b7e8-d188d1e43c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_3 = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,5))\n",
    "\n",
    "train_features_3 = vectorizer_3.fit_transform(data[mask_train][\"lemmatized_nltk\"])\n",
    "test_features_3 = vectorizer_3.transform(data[mask_test][\"lemmatized_nltk\"])\n",
    "\n",
    "words_3 = vectorizer_3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471beb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "4- and 5-grams seems overkill, but cool anyway!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e05cc9-f3ab-4eb2-9ce1-6f6a23e123b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stemming nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38324e0a-5f38-4d4c-ac9d-8b96951349f3",
   "metadata": {},
   "source": [
    "Preprocess texts using stemming method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4809e2-da96-4e4e-b872-63b42676cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a stmming module \n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming_nltk(text):\n",
    "    text_stemmed = \" \".join( # join lemmatized words back into text\n",
    "        [ps.stem(token) for token in word_tokenize(text)] # get lemmas for each word in the text\n",
    "    )\n",
    "    return text_stemmed\n",
    "\n",
    "data[\"stemmed_nltk\"] = data[\"review_norm\"].apply(stemming_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61048c83-74df-494f-aeb7-98641c020ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how stemming has worked\n",
    "for index in data.sample(5).index:\n",
    "    print(\"NORM: \", data.loc[index, \"review_norm\"][:100])\n",
    "    print(\"STEM: \", data.loc[index, \"stemmed_nltk\"][:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de0130-f6ae-468e-8132-02204db7bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize texts\n",
    "vectorizer_4 = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "train_features_4 = vectorizer_4.fit_transform(data[mask_train][\"stemmed_nltk\"])\n",
    "test_features_4 = vectorizer_4.transform(data[mask_test][\"stemmed_nltk\"])\n",
    "\n",
    "words_4 = vectorizer_4.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f398c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright, stemming is another way to normalize text data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642f295-d6b1-4cda-9823-4761be0b2f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e22a1c-b010-4b23-af91-59405dba5df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77f1a7-0eff-4a61-a63b-596adb7189d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):\n",
    "    \n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    for text in tqdm(texts, disable=disable_progress_bar):\n",
    "        ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        padded = np.array(ids + [0] * (max_length - len(ids)))\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        ids_list.append(padded)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        \n",
    "    if force_device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(force_device)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    if not disable_progress_bar:\n",
    "        print(\"Using the {} device.\".format(device))\n",
    "        \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar):\n",
    "        \n",
    "        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch)\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "        \n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f7e40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The code for generating BERT embeddings is correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119f6bd-fea4-430c-966a-f468b7f786e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_5 = get_bert_embeddings(data[mask_train][\"review_norm\"], force_device=\"mps\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19414a3-7c14-4918-8119-b2808f6725b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_5 = get_bert_embeddings(data[mask_test][\"review_norm\"], force_device=\"mps\", batch_size=64,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ad3a5-159c-4cca-865c-34b5e363e07b",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Model 0: Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc381ed5-5571-41d9-827f-c6647a59417f",
   "metadata": {},
   "source": [
    "Luckily, the whole dataset is already divided into train/test one part (*the corresponding flag is 'ds_part'*). Now store the target and features into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be310b1-2d65-496e-bf54-a875ebfc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target = data[mask_train][\"review_norm\"], data[mask_train][\"pos\"]\n",
    "test_features, test_target = data[mask_test][\"review_norm\"], data[mask_test][\"pos\"]\n",
    "\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62cf90-82e4-4004-8ec9-e3dd1b48418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a dummy model for predicting classes by random\n",
    "dummy_model = DummyClassifier(strategy=\"uniform\", random_state=555).fit(train_features, train_target)\n",
    "# check the classes balance of the predicted values\n",
    "print(\"Dummy predictions counts:\")\n",
    "pd.Series(dummy_model.predict(test_features)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcd177-87c4-4894-9b36-cdad810c4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dummy_model, train_features, train_target, test_features, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86006a1d-7e5d-40fd-9195-0df83ce20320",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "We have a F1 score of 0.67 using random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd00698",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright, there is a simple baseline\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9dea5-d247-4605-99d6-800f01e4fc87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model I: LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81e76e-c042-44b5-b928-012c69100f10",
   "metadata": {},
   "source": [
    "#### LR and NLTK lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fb7eb-8c0b-4e80-b597-631395c2b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lr_1 = LogisticRegression().fit(train_features_1, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_1, train_features_1, train_target, test_features_1, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a5c2d-851e-4663-8d31-986ed1f37a8a",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "The LogisticRegression model seem to work quite well. Try to improve using different text preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea081d7-85d3-43ce-9a53-6aefc8edab56",
   "metadata": {},
   "source": [
    "#### LR and spaCy lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bec5ec-e72e-4c9d-933e-eefbfc664f08",
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialize and fit the model again\n",
    "lr_2 = LogisticRegression().fit(train_features_2, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_2, train_features_2, train_target, test_features_2, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4afea-44dd-4609-bf78-daf9c9e59ff3",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "When using spaCy package for lemmatization the model performs slightly worse. It also takes much more time to preprocess texts with spaCy than it is when using nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ff83d-29f7-4be9-b178-248042948efd",
   "metadata": {},
   "source": [
    "#### LR and NLTK n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573368a-7a46-465e-b187-8436d7b16946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lr_3 = LogisticRegression().fit(train_features_3, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_3, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcf89c-9feb-42d1-9b72-168940f48021",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "When using n_grams the model gets overfitted. Try different hyperparameters to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d9a93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Yep, the ngram range is a better hyperparameter to tune here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b79ede-ac4d-4ab5-acf6-da470c3d55d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_31 = LogisticRegression()\n",
    "\n",
    "# parameters grid \n",
    "parameters = dict(C=(1,10,100,1000))\n",
    "\n",
    "# initialize and fit the gridsearch module\n",
    "lr_31 = GridSearchCV(lr_31, parameters, cv=3, verbose=0, scoring=\"f1\").fit(train_features_3, train_target)\n",
    "\n",
    "# get the best parameters\n",
    "lr_31.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd1b08-3fd4-4e7e-a0f3-8937569d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model with the regualrization parameter updated\n",
    "lr_31 = LogisticRegression(C=1000).fit(train_features_3, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_31, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c411ecf-8bf9-4c0d-8fe6-09c40da0b19c",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Now, the model knows the data perfectly, but the key metrics on the test set are the same as it was with NLTK lemmatization text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc57646-486f-4ba3-be8f-ea2c08a6d994",
   "metadata": {},
   "source": [
    "#### LR and NLTK stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fbd06-d267-4d73-8b18-5335d5412c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lr_4 = LogisticRegression().fit(train_features_4, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_4, train_features_4, train_target, test_features_4, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960d590-9f10-4930-a88b-a2f0c0613241",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "When texts preprocessed with the stemming method, the LR model yields the same results as it is when using lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87cd32-def4-4744-8fdd-36e84a3ed945",
   "metadata": {},
   "source": [
    "#### LR and BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7964e5-1e11-4f40-aefe-e73ddaf504f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lr_5 = LogisticRegression().fit(train_features_5, train_target)\n",
    "# evaluate the model\n",
    "evaluate_model(lr_5, train_features_5, train_target, test_features_5, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a029ea-0d46-494a-8fff-4fe5f9cb0e77",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "When using BERT embeddings the model does not overfit on graphs, the curves for the test data are very close to those for the training set. But the overall performance on the test set is a bit worse that with previous processing methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce824b9-3416-4ff4-bebc-a49aaf93040b",
   "metadata": {},
   "source": [
    "### Model 2: RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bde83-17fc-43db-a4b2-3d815684cc85",
   "metadata": {},
   "source": [
    "#### RF and NLTK lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047679d1-edd6-4f86-8a8e-c4a9039526ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_1 = RandomForestClassifier(random_state=12345, n_jobs=-1).fit(train_features_1, train_target)\n",
    "\n",
    "evaluate_model(forest_1, train_features_1, train_target, test_features_1, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040faee5-a328-40d9-9a2c-ad7db9fadf0d",
   "metadata": {},
   "source": [
    "Try to find better hyperparameters >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc07d67-e3db-456e-8276-8777d2ce2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_best = 0\n",
    "forest_11 = None\n",
    "\n",
    "best_depth = 0\n",
    "best_split = 0\n",
    "best_leaf = 0\n",
    "best_n_est = 0\n",
    "\n",
    "for depth in tqdm(range(30, 101, 10)):\n",
    "    for split in (2,4,8,16):\n",
    "        for leaf in (1,2,5):\n",
    "                \n",
    "            forest = RandomForestClassifier(\n",
    "                random_state=12345,\n",
    "                n_jobs=-1,\n",
    "                max_depth=depth,\n",
    "                min_samples_split=split,\n",
    "                min_samples_leaf=leaf\n",
    "            )\n",
    "\n",
    "            f1_scores = cross_val_score(\n",
    "                forest,\n",
    "                train_features_1,\n",
    "                train_target,\n",
    "                scoring=\"f1\",\n",
    "                cv=5,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            f1_average = np.mean(f1_scores)\n",
    "\n",
    "            if f1_average > f1_best:\n",
    "\n",
    "                f1_best = f1_average\n",
    "                forest_11 = forest\n",
    "                best_depth = depth\n",
    "                best_split = split\n",
    "                best_leaf = leaf\n",
    "                    \n",
    "print(\n",
    "    \"Best params: \",\n",
    "    \"\\nmax depth: \", best_depth,\n",
    "    \"\\nmin_samples_split: \", best_split,\n",
    "    \"\\nmin_samples_leaf: \", best_leaf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eff9a3-3810-40d0-af3a-962003c33b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_11.fit(train_features_1, train_target)\n",
    "\n",
    "evaluate_model(forest_11, train_features_1, train_target, test_features_1, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb1ba2-84e8-4e37-b138-8f43dfb87137",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "\n",
    "Not much of improvement on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4addca2-930e-49cd-a4fc-ec7e956965c6",
   "metadata": {},
   "source": [
    "#### RF and n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d16e07-b6f5-4201-b4f6-5f6aa58450bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_3 = RandomForestClassifier(random_state=12345, n_jobs=-1).fit(train_features_3, train_target)\n",
    "\n",
    "evaluate_model(forest_3, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300d53a-6917-42ee-9faf-b557f09c1b96",
   "metadata": {},
   "source": [
    "#### RF and BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f80639-981f-4671-8916-1e395a8cc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_5 = RandomForestClassifier(random_state=12345, n_jobs=-1).fit(train_features_5, train_target)\n",
    "\n",
    "evaluate_model(forest_5, train_features_5, train_target, test_features_5, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92280b84-d4d7-45e2-b51f-7f60fe6e585d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model 3: LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702c791-f1eb-4435-80c6-7777c541576e",
   "metadata": {},
   "source": [
    "#### LGBM and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d079613-f856-495f-9496-6aed440d76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_1 = LGBMClassifier(random_state=1345, n_jobs=-1).fit(train_features_1, train_target)\n",
    "\n",
    "evaluate_model(lgbm_1, train_features_1, train_target, test_features_1, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd3eb7-2767-44cd-9073-0f2f8cb5a35b",
   "metadata": {},
   "source": [
    "#### LGBM and n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47c046-0813-46e5-8549-9fac33d0e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_3 = LGBMClassifier(random_state=1345, n_jobs=-1).fit(train_features_3, train_target)\n",
    "\n",
    "evaluate_model(lgbm_3, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f9c08-454c-474a-be8e-e1d244740a0b",
   "metadata": {},
   "source": [
    "#### LGBM and BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f595a24-034e-46c8-bfbf-507da6dc1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_5 = LGBMClassifier(random_state=1345, n_jobs=-1).fit(train_features_5, train_target)\n",
    "\n",
    "evaluate_model(lgbm_5, train_features_5, train_target, test_features_5, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a09e43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Great, you tried different models with differently preprocessed texts and tuned the model's hyperparameters using cross-validation. Note that strictly speaking the test set should only be used to evaluate the final model to get an unbiased estimate of the final model's generalization performance. It would be better to use a separate validation set or cross-validation (although it would be a bit trickier to have similar visualizations for the metrics) instead for evaluation of different models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b9f37-408a-4dd2-be40-6420eb058b3c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71b081-c6f3-49e2-81a8-b5bfbcb800af",
   "metadata": {},
   "source": [
    "## New Reviews Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50e536-b4f2-4400-8af5-ddd17a1412e6",
   "metadata": {},
   "source": [
    "Use some reviews and star rating evaluations from Google Audience reviews to test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b39929-20fd-4461-b03c-c104103829c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = pd.DataFrame(\n",
    "    {\"review\":[\n",
    "        \"I like hard science fiction, when fantasy about possible events is grounded on current scientific facts. This movie does a good job in being fantastic enough, but not far from reality. With that said, I didn't really understand the motivation of the main character. She was presented as a very smart, wise person whose decisions made a huge impact on humanity. However, in the end, with all her knowledge, abilities, and absolute awareness, she makes an irrational, illogical, and unreasonable decision which causes suffering to two of her closest people. So, after all, the amazing events of the main part of the movie, are dimmed with this silly-human ending. Shame! But I like the movie, after all if I were me, I would watch it again.\",\n",
    "        \"I have watched every marvel movie, loved most of them, this Thor movie very disappointing, like a really bad B grade sci-fi from syfy channel, I don't blame the actors, Hemsworth & Bale are great actors, script writers and director are to blame. It was like a poorly scripted 80s movie, meant to be funny but just sad, gaps in the story line that didn't add up, trying not to give away the plot for those who may be thinking of seeing the movie. Yes there has always been a humour element to the marvel movies, this one seriously missed the mark, it was  reminiscent of a terrible spoof Thor movie. 3 of us went to see the movie, we were looking at each other with an expression of what is going on. 30 mins into the movie, we decided to go outside, spoke to a couple of the cinema staff who all said your not the first ones to come out, general opinion is the movie is bizarre and waste of money. We debated on going back in to watch a bit more to see if it gets any better, tried again it didn't get any better, Russell Crowe playing the part of Zeus King of the gods, not the image of you would expect, more like a really overweight grandad in a tutu prancing around.\",\n",
    "        \"Truly an enjoyable film, in some ways it felt different from most MCU films, and while it didn’t turn out to be as groundbreaking to critics as it’s predecessor, it proves to be the perfect superhero movie for the summer. Many others before me say that it didn’t have enough story, but let me tell you what it lacks in a “stellar” story it more than makes up for in action, humor, outstanding visuals, pop culture references, cameos, and overall creativity. While I was a little surprised not seeing Gorr do as much god butchering on screen, Christian Bale makes up for the character’s limited screen time through a powerful and albeit creepy performance, one scene in particular reminiscent of Pennywise from It, and his powers are unbelievably awesome. My only real complaints is are there is too much focus on the love story between Thor and Jane Foster, the first part of the film feels somewhat rushed (though I did enjoy the action packed ‘80s style fight), and of course without Loki, it just didn’t feel the same. Best of all, the mid and post credits scenes make me optimistic for the future of both Thor’s story and the MCU overall. My complaints are few and my thumbs are all the way up to the heavens to join the gods themselves 😇. Highly recommend this film, and please, don’t worry about what others think, go see it and make you’re own opinion ;). Also, Russell Crowe as Zeus was both powerful and hysterical, respect for holding the Greek accent 👍\",\n",
    "        \"I still play it even today. The graphics are amazing for a 2010 game and the story and missions are so well done. It’s incredibly fun with 2 players and since there are so many things to do you never get bored. There is an entire story mode that takes a decent chunk of time to finish and then the toy box mode Has TONS more missions and a little story of its own. You unlock cars. Rideable animals. (Bullseye, dragons),weapons and way more things!. This game will always have a special place in my heart and I’m lucky I found it so many years ago. Honestly one of the best games I’ve ever played I even recommend it today. You get bored of the new games these days it’s nice to just go back and enjoy the old days\",\n",
    "        \"Antman as a hero and a movie is underrated. Scott may be weird (which makes him different than the other Avengers and more funny) but his  ‘powers’ are really cool! This movie has action, great visuals, a well-written script, and has a few humorous lines/scenes. Honestly I wasn’t sure it was worth watching but I will now 100% recommend it to Marvel fans. It doesn’t have as much blood/gore, swearing, or violence as other Marvel movies, which can be a good or bad thing, but I think any Marvel fan should have no trouble enjoying it.\",\n",
    "        \"Great film, new cast is great, my only thing is that there should have been way, way more action scenes. Think about the Matrix Reloaded, they built a freeway and there was a 30min car vs bike chase against the agents, Morpheus, the key maker and Trinity on a Ducatti no less, with Neo flying back to save them all at the last second before the two lorries collided in a huge explosion, in bullet time with each other. That scene still blows my mind to this day! In this one Neo can't even fly, there is way too much backstory, most people know the story of the Matrix already, so it needed more action and special effects. John Wick 3 Parrabellum was great with Keanu Reeves for that very reason it was full of gun crazed action from start to finish. If you're a fan of the original Matrix and in a nostalgic mood it's a must watch, but I can't help but think they will need to back up (no pun intended) the Matrix Resurrections with another film for newcomers to the franchise, who will be totally confused with the back and forth awkward slowness of this version of the Matrix and lack of action, fight scenes and passion the first Trilogy of Matrix films had to offer. Enjoy!\",\n",
    "        \"I love the Matrix movies, and this one I thought was pretty good as well.  It’s been so long since I saw the trilogy that I was a little lost at first, mainly cause I had really forgotten what all happened to everyone, but after a refresher I was up to speed.  Keanu Reeves is great, but what I found that really surprised me is Neil Patrick Harris, he played that part remarkably well.  All in all, if you are a fan of the Matrix movies, as I am, you will like this movie too.  I hope this is the first of another possible trilogy and the story will continue on.  Not trying to disagree with those who didn’t like it, everyone has their own opinions, but maybe many fans was satisfied with how the trilogy concluded.  I could understand that point of view as well, and that some great things so just stay great and to leave it as it was, but hey, it makes for great conversation pieces.  The only negative I will say about the movie without giving any spoilers is that it was hard to follow along at the first, especially if you can’t remember the first three, so my suggestion is to maybe rewatch or watch for the first time, the first three movies and then flow into this new one. Just my opinion, everyone enjoy and god bless.\",\n",
    "        \"This movie presented the whole story about the hero saving his lady and making her fall in love again ...in a very Nolanistic way. The action scenes felt very real but not of the level of the movie which is being made in the 2020s. the whole movie had this nostalgic effect to it by which you can make all your OG fans from the OF Matrix decade. This movie explained a lot of things that made people confused from watching the 2nd & 3rd parts of Quadrology. Loved the whole concept of the dream, use of your brains and imagination with your reality related to it in parallel... keep this concept n any type of movie and I am gonna love it, No doubt. But yes, Nothing can match the level - The Matrix(1999). And yes can we please talk about Casting Jonathan Groff as Mr. Smith, Omg this was soo good to see him After the series- Mindhunters.This film made me more eager for the 3rd season.P.S- The Cameo of Chelsea (Cat) in this movie was so unexpected and also the last 30 secs post credit scene was the best thing About the movie\",\n",
    "        \"As a life long Matrix fan I was busting to see Resurrections 😀 I enjoyed it because the concepts and philosophy behind true reality intrigue me 🤔 If you are young and just go to the cinema to watch this film you will be so very DISAPPOINTED 😷 You will never be able to follow the plot. If you have not seen the previous 3 Matrix films you will subsequently fall asleep 🛏️ due to induced boardem. Even worse if you wanted great martial arts combat scenes 🥋 you are not getting any of those either 😤 All the action scenes were open to rubbish camera angles and a lack of awe and inspiration 🤯 The book 'Phenomenon' sold on Amazon 'Before Conception and Beyond' was Matrix 4 in many ways but unlike Resurrections ⚰️ the book takes you on the Greatest Adventure Ever Experienced 💡which cannot be said for the film sadly 🛀 The Phenomenon book plot is a very similar plot to the film and smashes into Matrix 5 'The Paradox' At least with the book 📚 'Phenomenon' you could put it down and raid the fridge for food 🌮and beers🍻. Honestly though 🤔 worth a watch if you are a true Matrix fan. If not honestly DON'T bother keep your money and buy the book. It will last longer and be more entertaining on your settee 🛋️ 📺  Hope you have enjoyed my review for you. Now it's time ☎️ for you to choose which pill 💊💊 to take. From a very rainy 🌧️ UK 🇬🇧 have a nice day 🌈🎱\",\n",
    "        \"If you have seen the trilogy + you like lot of actions then their is a huge chance that you will be disappointed from this movie. But this movie is almost a decent sequel I would say because it provide us the depth into the characters of not only Neo but also Trinity. This movie gives equal value to both of them and is MORE OF A LOVE STORY than an action packed superhero type movie- so if you don't expected that from the movie you will hate it like most of the others are. For me, the climax and ending satisfied me and I expect that another sequel (if it comes ever) will have all that great fight what we were expecting from this movie. Otherwise the first two parts will only remain favourite for me forever 💗\",\n",
    "        \"The Matrix: Resurrections sucks! it's worst Matrix movie ever and one of worst movies of 2021 also it did not feel like Matrix movie at all! The only good stuff I like was acting by some cast were great like Keanu Reeves was great as always as Neo along with Carrie-Anne Moss as Trinity, Jada Pinkett Smith as Niobe, and Priyanka Chopra as Sati, I like new character name Bugs who was played by Jessica Henwick even though it was confusing of where she came from, visuals effects were good, music was great, and fight scene were okay but mainly poorly. But bad stuff I hate recast of Morpheus who was played by Yahya Abdul-Mateen II being because Laurence Fishburne will always be Morpheus so it was stupid move, came out no where of villain name The Analyst when he saw Neo and Trinity dying in Revolutions, some of new powers did not make any sense at all like Analyst's power to slow down time, I hate they underuse Trinity, I hate how this movie rehash of first Matrix, I hate they brought back Agent Smith to life no offense he is defiantly one best cinematic villain especially that he was played by Hugo Weaving from The Matrix Trilogy but it was bad idea to bring him back to life and should stay dead also Jonathan Groff was bad choice to Agent Smith, ending was so bad, and my most biggest problem of this movie was Neo being video game maker and made The Matrix Trilogy and was making 4th one and it so stupid and bad! So yeah I think this movie is very unnecessary sequel and that Revolution was great ending to franchise. But because some few stuff that I like from movie that I had mention I would give this movie a 3/10 and I would defiantly watch 2 first sequels to The Matrix over 4th sequel.\"\n",
    "    ],\n",
    "    \"stars\":[4, 1, 4, 3, 5, 3, 5, 2, 4, 3, 2]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81a933-d6e4-4070-9aa2-366e3609adb8",
   "metadata": {},
   "source": [
    "#### Normalize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26160d7-857a-447f-abd4-1718293d3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews[\"review_norm\"] = [\n",
    "    \" \".join( # split and join to remove extra spaces made after substitution\n",
    "        re.sub(r\"[^a-z']\",\" \", text.lower()).split() # substitute all characters      \n",
    "    )                                                # that don't fit the pattern\n",
    "    for text \n",
    "    in new_reviews[\"review\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af3924-a1c2-4b92-a6a6-91cfed254296",
   "metadata": {},
   "source": [
    "#### LR and NLTK lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a2e61-1e3f-4cf2-ae7f-1b5eed9043ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "corpus = new_reviews[\"review_norm\"].apply(lemmatize_nltk)\n",
    "stars = new_reviews[\"stars\"]\n",
    "# give vectors\n",
    "vectors = vectorizer_1.transform(corpus)\n",
    "# get predictions\n",
    "new_reviews_prob_pred = lr_1.predict_proba(vectors)[:, 1]\n",
    "\n",
    "for i, review in enumerate(new_reviews[\"review\"].str.slice(0,100)):\n",
    "    print(\"{} stars,  {:.2f} pred | {}\".format(stars[i],new_reviews_prob_pred[i], review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090d477-1ace-4559-a660-1f97226c20dd",
   "metadata": {},
   "source": [
    "#### LR and BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98973297-17c7-4c42-814e-a25aed2a4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_bert = get_bert_embeddings(new_reviews[\"review_norm\"], force_device=\"mps\", batch_size=64)\n",
    "\n",
    "# get predictions\n",
    "new_reviews_prob_pred = lr_5.predict_proba(vectors_bert)[:, 1]\n",
    "\n",
    "for i, review in enumerate(new_reviews[\"review\"].str.slice(0,100)):\n",
    "    print(\"{} stars,  {:.2f} pred | {}\".format(stars[i],new_reviews_prob_pred[i], review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b79a48-98ed-4936-9511-f7ae6ad2e0ac",
   "metadata": {},
   "source": [
    "#### RF and NLTK lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52ed25-6754-4742-a502-badaa39ada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "new_reviews_prob_pred = forest_11.predict_proba(vectors)[:, 1]\n",
    "\n",
    "for i, review in enumerate(new_reviews[\"review\"].str.slice(0,100)):\n",
    "    print(\"{} stars,  {:.2f} pred | {}\".format(stars[i],new_reviews_prob_pred[i], review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a8c42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The models were applied to custom reviews correctly\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd136-da36-4976-9d78-e1259ec56fc2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b6351-a49b-4045-b773-176067bb9892",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad752bc3-c0bf-4301-906a-6ebb332936fe",
   "metadata": {},
   "source": [
    "1. **Exploratory data analysis** shows that the dataset is split into the train and test parts evenly: the target variable classes are balanced, the distributions of ratings, number of reviews, and dates in both sets are similar.   \n",
    "\n",
    "2. For **preprocessing texts** the NLTK package shows both better quality(when tested on a model) and the speed of preprocessing than that with spaCy. Stemming shows no obvious difference compared to lemmatization. On the test set, using n_grams shows no improvement compared to single word TF-IDF vectorizing. Using BERT encoding gives slightly worse results than simple lemmatization/stemming + TF-IDF techniques. \n",
    "\n",
    "3. Among all **models** and text preprocessing techniques a simple logistic regression with either stemming or lemmatization, plus TF-IDF vectorizing showed the best F1 score of `0.88`, which is 24% better than the baseline. Using more complex models for both text processing and making predictions doesn't give better results.\n",
    "\n",
    "4. When testing the model's **performance on new examples**, subjectively, the *lemmatization + TF-IDF + LR* setup yields the most reasonable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3309b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Conclusions look good! Note that the custom reviews section is intended for illustration purposes: it doesn't make sense to use them to judge the models' performance, when we have a much more representative test set\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
