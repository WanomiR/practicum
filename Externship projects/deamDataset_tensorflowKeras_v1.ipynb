{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WanomiR/practicum/blob/main/Externship%20projects/deamDataset_tensorflowKeras_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f165ae-98ad-4770-acaf-e246667103ea",
      "metadata": {
        "id": "50f165ae-98ad-4770-acaf-e246667103ea"
      },
      "source": [
        "# ***Music Valence Prediction: Classic Models***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aaccb09-f3ec-46cc-9622-5fbd23dc8df1",
      "metadata": {
        "id": "0aaccb09-f3ec-46cc-9622-5fbd23dc8df1"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da35e2c5-1b1e-4807-b65e-801f7b7bd738",
      "metadata": {
        "tags": [],
        "toc-hr-collapsed": true,
        "id": "da35e2c5-1b1e-4807-b65e-801f7b7bd738"
      },
      "source": [
        "## Project Description"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879349dc-3aac-49f3-aa52-349887b7a196",
      "metadata": {
        "id": "879349dc-3aac-49f3-aa52-349887b7a196"
      },
      "source": [
        "### The Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bad051-51a8-413f-81d3-b860e37111ba",
      "metadata": {
        "id": "d6bad051-51a8-413f-81d3-b860e37111ba"
      },
      "source": [
        "The goal of this task is to develop a Python-based module to predict the valence of newly released pop songs.   Two approaches are to use as an input: \n",
        "1) the audio data (e.g. `.wav` files) \n",
        "2) the songs lyrics\n",
        "\n",
        "Publicly available datasets can be used for training and testing. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c32d2d1-a58e-4e18-bf41-9fba0181de94",
      "metadata": {
        "tags": [],
        "id": "5c32d2d1-a58e-4e18-bf41-9fba0181de94"
      },
      "source": [
        "### Audio Signal Features\n",
        "<a id=\"audio-features\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c829f23-d256-44de-a214-5074699eaf87",
      "metadata": {
        "id": "2c829f23-d256-44de-a214-5074699eaf87"
      },
      "source": [
        "In this version of the project, the approach of the following research paper will be implemented: [Measuring national mood with music](https://link.springer.com/article/10.3758/s13428-021-01747-7). Specifically, the same [set of features](https://static-content.springer.com/esm/art%3A10.3758%2Fs13428-021-01747-7/MediaObjects/13428_2021_1747_MOESM1_ESM.pdf) will be tried to extract. These features are:\n",
        "- [x] Spectral Centroid;\n",
        "- [x] Spectral Rolloff;\n",
        "- [x] Spectral Contrast — ~~7 bands~~ *(kept the default 6 bands instead)*;\n",
        "- [x] Mel-Frequency Cepstrum Coefficients (MFCC) — 24 coefficients;\n",
        "- [x] Zero Crossing Rate;\n",
        "- [x] Chroma Energy Normalized Statistics (CENS) — 12 chroma;\n",
        "- [x] Beat Per Minute (BPM);\n",
        "- [x] Root Mean Square (RMS);\n",
        "- [ ] ~~Spectral Flux~~*(couldn't extract)*;\n",
        "- [ ] ~~Onset Rate~~(couldn't extract);\n",
        "- [ ] ~~High Frequency Content (HFC)~~ *(couldn't extract, seemingly the same as Spectral Rolooff with high frequency)*;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58a220c-bf16-46c4-a819-7aaac7f0a024",
      "metadata": {
        "id": "a58a220c-bf16-46c4-a819-7aaac7f0a024"
      },
      "source": [
        "### The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2293414b-80d4-44ba-8988-5a280f8b8726",
      "metadata": {
        "id": "2293414b-80d4-44ba-8988-5a280f8b8726"
      },
      "source": [
        "Fr this task, the more recent version of this dataset will be used: [DEAM dataset](https://cvml.unige.ch/databases/DEAM/). It seemingly has a more diverse range of music genres, including electronic and Hip-hop music, which are absent in the previous version."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e12007-e50e-42dc-93a3-906fed0eaf0e",
      "metadata": {
        "id": "a7e12007-e50e-42dc-93a3-906fed0eaf0e"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9718bd91-558d-4ba8-968b-ee2b1d9d7eb5",
      "metadata": {
        "tags": [],
        "id": "9718bd91-558d-4ba8-968b-ee2b1d9d7eb5"
      },
      "source": [
        "## Imports and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4375af81-7987-4c04-bde9-8cd7b5510291",
      "metadata": {
        "id": "4375af81-7987-4c04-bde9-8cd7b5510291"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "\n",
        "from scipy import stats as st\n",
        "from scipy.signal import get_window\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "from datetime import date\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "83ff277f-6abc-44db-b7ca-68662287b06b",
      "metadata": {
        "id": "83ff277f-6abc-44db-b7ca-68662287b06b"
      },
      "outputs": [],
      "source": [
        "# set up some parameters for plots in this notebook\n",
        "plt.style.use(\"seaborn-paper\")\n",
        "sns.set_style(\"dark\", {\"axes.facecolor\": \".95\"})\n",
        "sns.set_palette(\"mako\")\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
        "%config InlineBackend.figure_format = \"retina\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ffc8acb9-4135-4908-b155-a52718545a44",
      "metadata": {
        "id": "ffc8acb9-4135-4908-b155-a52718545a44"
      },
      "outputs": [],
      "source": [
        "# save random state number in advance\n",
        "seed = 12345"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062af143-ae31-41e1-bf1f-7d96524b2f43",
      "metadata": {
        "id": "062af143-ae31-41e1-bf1f-7d96524b2f43"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c4fa27-6d1d-4ea7-8e69-edfcce164f36",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "toc-hr-collapsed": true,
        "id": "96c4fa27-6d1d-4ea7-8e69-edfcce164f36"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f755a2-ca2e-4c29-86c2-b21078a28e31",
      "metadata": {
        "tags": [],
        "id": "b1f755a2-ca2e-4c29-86c2-b21078a28e31"
      },
      "source": [
        "### Annotations data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb1b7e1f-7a62-453f-ae9a-4d4cd9167516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "bb1b7e1f-7a62-453f-ae9a-4d4cd9167516",
        "outputId": "00652f16-494c-4c9a-ce93-baae06de3612"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d2f5c66dd443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the annotations file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/static_annotations_averaged_songs_1_2000.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/static_annotations_averaged_songs_1_2000.csv'"
          ]
        }
      ],
      "source": [
        "# read the annotations file\n",
        "annot = pd.read_csv(\"datasets/static_annotations_averaged_songs_1_2000.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae00533-04f7-4669-bad7-2e8fa1137f6c",
      "metadata": {
        "id": "2ae00533-04f7-4669-bad7-2e8fa1137f6c"
      },
      "outputs": [],
      "source": [
        "# show general info\n",
        "annot.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134924e7-16e6-4da3-bba2-dc71279d5938",
      "metadata": {
        "id": "134924e7-16e6-4da3-bba2-dc71279d5938"
      },
      "outputs": [],
      "source": [
        "# print several random rows\n",
        "annot.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74a7419-3e5c-4672-839f-5cd0cce0ce2c",
      "metadata": {
        "id": "b74a7419-3e5c-4672-839f-5cd0cce0ce2c"
      },
      "source": [
        "The column names contain \"mean\" and \"std\" due to the labeling procedure: each song was labeled by 10 people. The average value of all rates was then taken and the standard deviation was calculated. For this project, this information is not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454fa87b-26a1-47d4-8b15-d5f1edca9331",
      "metadata": {
        "id": "454fa87b-26a1-47d4-8b15-d5f1edca9331"
      },
      "outputs": [],
      "source": [
        "# the column names contain empty spaces, fix that\n",
        "annot.columns = [col.replace(\" \",\"\") for col in annot.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca2c3fbf-b3cd-4268-ad3a-aba3452ea7b7",
      "metadata": {
        "id": "ca2c3fbf-b3cd-4268-ad3a-aba3452ea7b7"
      },
      "outputs": [],
      "source": [
        "# drop extra columns\n",
        "annot.drop(columns=[\"valence_std\",\"arousal_std\"], inplace=True)\n",
        "# shortent the column names\n",
        "annot.rename(columns={\"valence_mean\":\"valence\", \"arousal_mean\":\"arousal\"}, inplace=True)\n",
        "annot.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecebd36-37a4-4499-b1e3-339e089ef85c",
      "metadata": {
        "tags": [],
        "id": "aecebd36-37a4-4499-b1e3-339e089ef85c"
      },
      "source": [
        "### Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d5265b-93af-43e8-ab5f-44f0c26f9fec",
      "metadata": {
        "id": "52d5265b-93af-43e8-ab5f-44f0c26f9fec"
      },
      "outputs": [],
      "source": [
        "# read the meatdata file\n",
        "meta = pd.read_csv(\"datasets/metadata_1_2000.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fcbb26f-8172-4ca6-b922-01703f6e8978",
      "metadata": {
        "id": "5fcbb26f-8172-4ca6-b922-01703f6e8978"
      },
      "outputs": [],
      "source": [
        "# show general info\n",
        "meta.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06373e8-30fe-433c-ba58-ace47e799a9b",
      "metadata": {
        "id": "a06373e8-30fe-433c-ba58-ace47e799a9b"
      },
      "source": [
        "The reason the `file_name` column contains missing values is that this file was compiled from two (with the use of MS Excel) and the second of these files didn't have this information. However, the file names can be derived from the `song_id` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3935cef3-42fa-42fe-84c4-dd8cc6166fbe",
      "metadata": {
        "id": "3935cef3-42fa-42fe-84c4-dd8cc6166fbe"
      },
      "outputs": [],
      "source": [
        "# print a sample of the meta set\n",
        "meta.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7253ecb3-328e-4199-9cb1-60576c37d170",
      "metadata": {
        "id": "7253ecb3-328e-4199-9cb1-60576c37d170"
      },
      "source": [
        "The segment points are noted differently for the first 744 song, than it is for the second 100. However, it is known that each audio file in this dataset is 45 seconds length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58808230-f771-4a4d-994c-20f78d635c43",
      "metadata": {
        "id": "58808230-f771-4a4d-994c-20f78d635c43"
      },
      "outputs": [],
      "source": [
        "# tidy up the column names\n",
        "meta.columns = [\"song_id\",\"file_name\",\"artist\",\"song_title\",\"segment_start\",\"segment_end\", \"genre\"]\n",
        "# fill in missing song titles\n",
        "meta[\"song_title\"].fillna(\"unknown\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f93646c-f6c3-4a41-b452-ae94785730d4",
      "metadata": {
        "id": "9f93646c-f6c3-4a41-b452-ae94785730d4"
      },
      "outputs": [],
      "source": [
        "# remove artifacts\n",
        "for col in [\"artist\",\"song_title\",\"genre\"]:\n",
        "    meta[col] = [re.sub(r\"\\t\", \"\", string) for string in meta[col]]\n",
        "\n",
        "# constract new file_name column\n",
        "meta[\"file_name\"] = meta[\"song_id\"].astype(str) + \".mp3\"\n",
        "\n",
        "# print sample to check\n",
        "meta.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cc0cb6-958f-4d1f-b611-d27a8205d7e4",
      "metadata": {
        "tags": [],
        "id": "84cc0cb6-958f-4d1f-b611-d27a8205d7e4"
      },
      "source": [
        "### Merge datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ffea0d-67cd-4381-844d-f7270457eb2f",
      "metadata": {
        "id": "61ffea0d-67cd-4381-844d-f7270457eb2f"
      },
      "outputs": [],
      "source": [
        "data = annot.merge(meta, on=\"song_id\",how=\"outer\")\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93939f0b-ec92-4020-9925-b7b2b29b41db",
      "metadata": {
        "id": "93939f0b-ec92-4020-9925-b7b2b29b41db"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07adc793-4cc1-46f9-a8ac-5c69d3837564",
      "metadata": {
        "id": "07adc793-4cc1-46f9-a8ac-5c69d3837564"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0159546f-c01c-4772-a402-1adc7fb5606e",
      "metadata": {
        "tags": [],
        "toc-hr-collapsed": true,
        "id": "0159546f-c01c-4772-a402-1adc7fb5606e"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a564c5cb-fc16-4346-a4f1-cb27b9483591",
      "metadata": {
        "id": "a564c5cb-fc16-4346-a4f1-cb27b9483591"
      },
      "source": [
        "### Valence vs arousal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba8251b-e6f5-4dd2-8838-0b4b0d5db391",
      "metadata": {
        "id": "fba8251b-e6f5-4dd2-8838-0b4b0d5db391"
      },
      "outputs": [],
      "source": [
        "# plot the valence and arousal values distribution\n",
        "plt.scatter(data[\"valence\"], data[\"arousal\"], alpha=.3)\n",
        "plt.title(\"Valence vs arousal scatter plot\", fontsize=14)\n",
        "plt.xlim([1,9])\n",
        "plt.ylim([1,9])\n",
        "plt.xlabel(\"Valence\")\n",
        "plt.ylabel(\"Arousal\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e43619-a8d8-4f07-8497-6db2fd1ce5a1",
      "metadata": {
        "id": "16e43619-a8d8-4f07-8497-6db2fd1ce5a1"
      },
      "source": [
        "There is a noticeable general correlation between these values, which is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4aa1b23-1f16-46a2-8682-d80e5cc366d0",
      "metadata": {
        "id": "b4aa1b23-1f16-46a2-8682-d80e5cc366d0"
      },
      "source": [
        "### Genre classes balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916884b7-91ec-4c99-8f1e-6eb9039c7ddc",
      "metadata": {
        "id": "916884b7-91ec-4c99-8f1e-6eb9039c7ddc"
      },
      "outputs": [],
      "source": [
        "# count the number of genres present in the dataset\n",
        "nunique_genres = data[\"genre\"].nunique()\n",
        "print(\"Unique genres in the dataset:  {}\\n\".format(nunique_genres))\n",
        "\n",
        "# select the top 10\n",
        "top10_genres = data[\"genre\"].value_counts()[:10]\n",
        "top10_genres.loc[\"Other\"] = data[\"genre\"].value_counts()[10:].sum()\n",
        "\n",
        "# plot a pie chart with genres ratio\n",
        "plt.figure(figsize=(6,6))\n",
        "labels = top10_genres.index\n",
        "plt.pie(top10_genres, labels=labels)\n",
        "plt.title(\"Proportion of genres in he dataset\", fontsize=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c220646-c1e9-41be-9b2e-f4eab4d2bb07",
      "metadata": {
        "id": "6c220646-c1e9-41be-9b2e-f4eab4d2bb07"
      },
      "source": [
        "The data contains a variety of music genres. Among the top-10 are present some genres related to this task: Pop, Hip-Hop, Rock, Pop-Rock, and Electronic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb1315a-7e12-4c36-ba2e-197498d88716",
      "metadata": {
        "id": "fdb1315a-7e12-4c36-ba2e-197498d88716"
      },
      "source": [
        "### Audio and waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3d8198-42b3-4276-9846-1f774132a463",
      "metadata": {
        "id": "bd3d8198-42b3-4276-9846-1f774132a463"
      },
      "outputs": [],
      "source": [
        "random_idx = np.random.randint(0, 1744)\n",
        "\n",
        "# select a random song from the dataset\n",
        "song = data.loc[random_idx, :]\n",
        "\n",
        "# load the file and print its sampling rate \n",
        "file_path = \"datasets/audio_files/\" + song[\"file_name\"]\n",
        "y, sr = librosa.load(file_path, duration=30) # select only 30 seconds\n",
        "# print info about this song\n",
        "print(f\"Sampling rate: {sr}\")\n",
        "print(song)\n",
        "\n",
        "# plot the wavefrom\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.waveshow(y, sr=sr) # plot a waveform and play the file\n",
        "plt.title(f'\"{song.song_title[:15]}\" by {song.artist}, {song.genre}')\n",
        "plt.legend([f\"song id {song.song_id}\"])\n",
        "plt.xlabel(\"Duration in seconds\")\n",
        "\n",
        "# output the audio\n",
        "ipd.Audio(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9d484f-e62c-4cc9-b152-f92c8c32c02e",
      "metadata": {
        "id": "ff9d484f-e62c-4cc9-b152-f92c8c32c02e"
      },
      "source": [
        "Some songs don't sound like they belong to the genre with which they were associated. For example, [song 402](https://freemusicarchive.org/music/Patrick_Lee/Bad_Panda_42/Quittin_Time/) is way more funk than electronic, but it was marked as Electronic in the dataset, and [song 43](https://freemusicarchive.org/music/The_Agrarians/Uncomfortably_Songwriter_Vol_VI/Puritans_Too_1938/) can barely be described as Blues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56352c0e-f912-4e86-9cf6-c3897fed84e1",
      "metadata": {
        "id": "56352c0e-f912-4e86-9cf6-c3897fed84e1"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f886cf-f526-452d-b469-27e574a9d101",
      "metadata": {
        "id": "e6f886cf-f526-452d-b469-27e574a9d101"
      },
      "source": [
        "## Audio Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dddc40d-a2b7-4995-bc3d-ec4a6d6f7d4d",
      "metadata": {
        "id": "3dddc40d-a2b7-4995-bc3d-ec4a6d6f7d4d"
      },
      "source": [
        "Look at some of the audio features available with `librosa` and build a features extraction procedure. The feature descriptions are mostly taken from [this text](https://blog.paperspace.com/music-genre-classification-using-librosa-and-pytorch/https://blog.paperspace.com/music-genre-classification-using-librosa-and-pytorch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ce9c6c-f170-43a9-b5da-1d46d529c901",
      "metadata": {
        "id": "a2ce9c6c-f170-43a9-b5da-1d46d529c901"
      },
      "source": [
        "### Mel Frequency Cepstral Coefficients (MFCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e16f19-422e-49d3-819b-d640ed3d96e3",
      "metadata": {
        "id": "38e16f19-422e-49d3-819b-d640ed3d96e3"
      },
      "source": [
        "MFCCs are Cepstral coefficients calculated by a discrete cosine transform applied to the power spectrum of a signal. The frequency bands of this spectrum are spaced logarithmically according to the Mel scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d24cce-0718-4999-9607-a18a9ddd2ce2",
      "metadata": {
        "id": "65d24cce-0718-4999-9607-a18a9ddd2ce2"
      },
      "outputs": [],
      "source": [
        "mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "plt.imshow(mfcc, interpolation=\"nearest\", aspect=\"auto\", cmap=\"mako\")\n",
        "plt.title(\"MFCC of the file {}\".format(song.file_name));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64068d9c-4d1a-4b7d-b976-b7bed678a91f",
      "metadata": {
        "id": "64068d9c-4d1a-4b7d-b976-b7bed678a91f"
      },
      "source": [
        "### Mel Spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f7db28-c91e-47bf-9234-b5ed98f80693",
      "metadata": {
        "id": "00f7db28-c91e-47bf-9234-b5ed98f80693"
      },
      "source": [
        "The Mel spectrogram is the standard spectrogram in the Mel scale, which is a perceptual scale of pitches that listeners perceive to be equally spaced from one another. The conversion from the frequency in hertz to the Mel scale is done using the following formula:\n",
        "$$\n",
        "m = 2595\\ \\text{log}_{10}(1 + \\frac{f}{700})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05339d6f-d655-4b24-9899-b86ce657d93b",
      "metadata": {
        "id": "05339d6f-d655-4b24-9899-b86ce657d93b"
      },
      "outputs": [],
      "source": [
        "melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)        # reverse cmap for better visability \n",
        "plt.imshow(melspectrogram, interpolation=\"nearest\", aspect=\"auto\", cmap=matplotlib.cm.get_cmap(\"mako_r\"))\n",
        "plt.title(\"Mel Cpectrogram for the file {}\".format(song.file_name));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70893f8f-b1cd-4529-a104-a965bf5009a9",
      "metadata": {
        "id": "70893f8f-b1cd-4529-a104-a965bf5009a9"
      },
      "source": [
        "### Chroma Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8885507d-77cc-4977-9410-f57dd234ea48",
      "metadata": {
        "id": "8885507d-77cc-4977-9410-f57dd234ea48"
      },
      "source": [
        "The Chroma features vector is constructed by having the full spectrum projected onto 12 bins that reflect the 12 unique semitones (or Chroma) of the musical octave: **C, C#, D, D#, E, F, F#, G, G#, A, A#, B**. This projection gives an intriguing and potent representation of music audio and is especially dependent on the music genre.  \n",
        "\n",
        "Since notes that are exactly one octave apart are perceived as being particularly similar in music, understanding the distribution of Chroma, even without knowing the absolute frequency (i.e., the original), can provide useful musical information about the audio and may even expose perceived musical similarities in the same music genre that are not visible in the original spectra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76213e2-ad01-419e-9865-b13018a33be6",
      "metadata": {
        "id": "c76213e2-ad01-419e-9865-b13018a33be6"
      },
      "outputs": [],
      "source": [
        "chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "plt.imshow(chroma,interpolation=\"nearest\", aspect=\"auto\", cmap=\"mako\")\n",
        "plt.title(\"Chroma vector of the auidio file {}\".format(song.file_name));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b6b8c9-a50c-4ef3-847c-3c8ee806df1a",
      "metadata": {
        "id": "e8b6b8c9-a50c-4ef3-847c-3c8ee806df1a"
      },
      "source": [
        "### Tonal Centroid Features (Tonnetz)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19732361-91db-4fe6-9683-006d86fb5462",
      "metadata": {
        "id": "19732361-91db-4fe6-9683-006d86fb5462"
      },
      "source": [
        "Tonnetz is calculated by projecting Chroma features onto a 6-dimensional basis representing the perfect fifth, minor third, and major third each as two dimensional coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ec4960-1b52-4dbf-9d7d-4a7efeaab46c",
      "metadata": {
        "id": "a1ec4960-1b52-4dbf-9d7d-4a7efeaab46c"
      },
      "outputs": [],
      "source": [
        "tntz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "plt.imshow(tntz, interpolation=\"nearest\", aspect=\"auto\", cmap=\"mako\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd432cf-b4bc-460d-87b6-f9fa08c9e544",
      "metadata": {
        "id": "8dd432cf-b4bc-460d-87b6-f9fa08c9e544"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef875c1-291b-44bd-b5d4-54936724361c",
      "metadata": {
        "id": "cef875c1-291b-44bd-b5d4-54936724361c"
      },
      "source": [
        "## Features preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388d61ec-33a4-4cd9-818e-8edbedaf1844",
      "metadata": {
        "id": "388d61ec-33a4-4cd9-818e-8edbedaf1844"
      },
      "source": [
        "### Extraction procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "170e82cb-fd2d-4b99-b15b-a990367f699c",
      "metadata": {
        "id": "170e82cb-fd2d-4b99-b15b-a990367f699c"
      },
      "outputs": [],
      "source": [
        "def extract_features(file_path):\n",
        "    \n",
        "    y, sr = librosa.load(file_path, duration=30) # select only 30 seconds \n",
        "    \n",
        "    # MFCC\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "    mfcc_mean = mfcc.mean(axis=1)\n",
        "    mfcc_var = mfcc.var(axis=1)\n",
        "    mfcc_min = mfcc.min(axis=1)\n",
        "    mfcc_max = mfcc.max(axis=1)\n",
        "    mfcc_feature = np.concatenate((mfcc_mean, mfcc_var, mfcc_min, mfcc_max))\n",
        "    \n",
        "    # Mel Spectrogram\n",
        "    melspec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    melspec_mean = melspec.mean(axis=1)\n",
        "    melspec_var = melspec.var(axis=1)\n",
        "    melspec_min = melspec.min(axis=1)\n",
        "    melspec_max = melspec.max(axis=1)\n",
        "    melspec_feature = np.concatenate(\n",
        "        (melspec_mean, mfcc_var, mfcc_min, mfcc_max)\n",
        "    )\n",
        "    \n",
        "    # Chroma vector\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    chroma_mean = chroma.mean(axis=1)\n",
        "    chroma_var = chroma.var(axis=1)\n",
        "    chroma_min = chroma.min(axis=1)\n",
        "    chroma_max = chroma.max(axis=1)\n",
        "    chroma_feature = np.concatenate(\n",
        "        (chroma_mean, chroma_var, chroma_min, chroma_max)\n",
        "    )\n",
        "    \n",
        "    # Tonnetz\n",
        "    tntz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "    tntz_mean = tntz.mean(axis=1)\n",
        "    tntz_var = tntz.var(axis=1)\n",
        "    tntz_min = tntz.min(axis=1)\n",
        "    tntz_max = tntz.max(axis=1)\n",
        "    tntz_feature = np.concatenate(\n",
        "        (tntz_mean, tntz_var, tntz_min, tntz_max)\n",
        "    )\n",
        "    \n",
        "    features = np.concatenate(\n",
        "        (mfcc_feature, melspec_feature, chroma_feature, tntz_feature)\n",
        "    )\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3ea9a50-e2ef-4d72-9246-35270a3dfe82",
      "metadata": {
        "id": "e3ea9a50-e2ef-4d72-9246-35270a3dfe82"
      },
      "source": [
        "### Extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f17d74d6-ec6c-4176-82a0-21aece9125b4",
      "metadata": {
        "id": "f17d74d6-ec6c-4176-82a0-21aece9125b4"
      },
      "outputs": [],
      "source": [
        "# features_df = []\n",
        "# for i in tqdm(range(len(data))):\n",
        "    \n",
        "#     file_name = data.loc[i, \"file_name\"]\n",
        "#     file_path = \"datasets/audio_files/\" + file_name\n",
        "#     features_df.append(extract_features(file_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0420f012-8eac-4ac9-80af-8fe50a599770",
      "metadata": {
        "id": "0420f012-8eac-4ac9-80af-8fe50a599770"
      },
      "outputs": [],
      "source": [
        "# features_df = pd.DataFrame(features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c206e07-17f6-4786-9c10-ef46cd878e3f",
      "metadata": {
        "id": "8c206e07-17f6-4786-9c10-ef46cd878e3f"
      },
      "outputs": [],
      "source": [
        "# features_df.to_csv(\"features_df_2022-12-26.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd7ea6c-c07c-419e-8755-fbc58d0409c7",
      "metadata": {
        "id": "1fd7ea6c-c07c-419e-8755-fbc58d0409c7"
      },
      "outputs": [],
      "source": [
        "features_df = pd.read_csv(\"features_df_2022-12-26.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9161391-bca9-4a89-af86-2ff2a0b97acb",
      "metadata": {
        "id": "d9161391-bca9-4a89-af86-2ff2a0b97acb"
      },
      "source": [
        "### Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7d9e3e-ecfb-4b4a-aaef-28513eb020aa",
      "metadata": {
        "id": "8a7d9e3e-ecfb-4b4a-aaef-28513eb020aa"
      },
      "outputs": [],
      "source": [
        "X = features_df.values\n",
        "y = data.valence.values\n",
        "\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size=.2, random_state=seed)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_, y_, test_size=.25, random_state=seed)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d126b0d-0bcf-4357-bcb2-18d24b1f3b12",
      "metadata": {
        "id": "0d126b0d-0bcf-4357-bcb2-18d24b1f3b12"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205bf476-7949-4e6f-bf7d-718bdc1eda19",
      "metadata": {
        "id": "205bf476-7949-4e6f-bf7d-718bdc1eda19"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334a344e-a0f7-419a-8fab-bc5f1656750b",
      "metadata": {
        "id": "334a344e-a0f7-419a-8fab-bc5f1656750b"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "model = Sequential()\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "# the first layer\n",
        "model.add(Dense(300, input_dim=340, activation=\"relu\"))\n",
        "model.add(Dropout(.1))\n",
        "# the second layer\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(.1))\n",
        "# the third layer\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "# the final layer\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"mean_squared_error\", \n",
        "    metrics=[\"mean_absolute_error\"], \n",
        "    optimizer=optimizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e8d44a-311a-4347-aee7-3a140ff99320",
      "metadata": {
        "id": "53e8d44a-311a-4347-aee7-3a140ff99320"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "num_epochs = 200\n",
        "num_batch_size = 32\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=\"saved_models/valence_predicition.hdf5\",\n",
        "                              verbose=1, save_best_only=True)\n",
        "\n",
        "# initialize callbacks\n",
        "earlystop = EarlyStopping(patience=20)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor=\"val_mean_absolute_error\", \n",
        "                                                patience=10, \n",
        "                                                verbose=1, \n",
        "                                                factor=0.5, \n",
        "                                                min_lr=0.00001)\n",
        "\n",
        "# fit the model\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=num_batch_size,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=[checkpointer, earlystop, learning_rate_reduction],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866fcba6-9de6-4cad-95ab-7acb3bf5811c",
      "metadata": {
        "id": "866fcba6-9de6-4cad-95ab-7acb3bf5811c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9 ARM 64 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}